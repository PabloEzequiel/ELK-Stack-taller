input {  
  file {
    # path => "${ELK_STACK_TALK_HOME}/Kibana/data/DATASET_03_2016.csv"
    path => "/Users/pabloinchausti/Desktop/DevOps/code/github/Pabloin/ELK-Stack/Kibana/data/DATASET_03_2016.4.csv"
    start_position => "beginning"

     # Para que resetee ... 
    sincedb_path => "/dev/null" 
  }
}
filter {  

# FECHA,HORA,WATERQ_Temp,WATERQ_pH,WATERQ_OD,WATERQ_OD_pc,DATASET
# 05/02/2016,13:15:00,13.79,7.42,9.75,94.45,DATASET_03_2016



  csv {
      separator => ","
      columns => ["FECHA","HORA","WATERQ_Temp","WATERQ_pH","WATERQ_OD","WATERQ_OD_pc","DATASET"]
  }
  

  mutate {
    add_field => { "fecha_time_stamp" => "%{FECHA} %{HORA}" }
  }

  date {

    #Â If you want to have "logtimestamp" become a time object,
    #    you need to add target => "logtimestamp" to your date filter block.
    # By default, the date filter overwrites the @timestamp field with the value of the matched field,
    #    in this case, logtimestamp's value. 
    # match => [ "FECHA", "dd/MM/yyyy" ]

    match => [ "fecha_time_stamp", "dd/MM/yyyy HH:mm:ss" ]
    #    match => [ "HORA", "HH:mm:ss" ]
    # target => "FECHA"
  }




 #mutate {convert => ["FECHA",  "float"]}
  mutate {convert => ["WATERQ_Temp",  "float"]}
  mutate {convert => ["WATERQ_pH",    "float"]}
  mutate {convert => ["WATERQ_OD",    "float"]}
  mutate {convert => ["WATERQ_OD_pc", "float"]}
}
output {  

    elasticsearch {
        hosts => "http://localhost:9200"
        index => "dataset-03-2016-v3-small-2"
    }
    
    stdout {}



}